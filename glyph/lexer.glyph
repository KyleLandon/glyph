# Minimal Glyph lexer
def tokenize(source: str) -> list:
    tokens = []
    i = 0
    while i < len(source):
        c = source[i]
        if c.isspace():
            i += 1
        elif c.isalpha() or c == '_':
            start = i
            while i < len(source) and (source[i].isalnum() or source[i] == '_'):
                i += 1
            tokens.append(('ID', source[start:i]))
        elif c.isdigit():
            start = i
            while i < len(source) and source[i].isdigit():
                i += 1
            tokens.append(('NUMBER', source[start:i]))
        elif c in '(){}[],:=+-*/':
            tokens.append(('PUNCT', c))
            i += 1
        else:
            # Skip unknown chars for now
            i += 1
    return tokens 